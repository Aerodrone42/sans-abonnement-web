
import { useState, useRef, useEffect } from 'react';
import { EnhancedChatGPTService } from '@/services/enhancedChatGptService';
import { SpeechSynthesisService } from '@/services/speechSynthesisService';

interface UseVoiceRecognitionProps {
  onTranscript: (text: string, field: string) => void;
  conversationMode: boolean;
  chatGPT: EnhancedChatGPTService | null;
}

interface ExtendedSpeechRecognition extends SpeechRecognition {
  onend: ((this: SpeechRecognition, ev: Event) => any) | null;
  onerror: ((this: SpeechRecognition, ev: SpeechRecognitionErrorEvent) => any) | null;
  onresult: ((this: SpeechRecognition, ev: SpeechRecognitionEvent) => any) | null;
}

export const useVoiceRecognition = ({ onTranscript, conversationMode, chatGPT }: UseVoiceRecognitionProps) => {
  // √âtats simplifi√©s
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState("");
  const [isProcessing, setIsProcessing] = useState(false);
  const [lastResponse, setLastResponse] = useState("");
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isConversationActive, setIsConversationActive] = useState(false);
  
  // Refs pour la gestion technique
  const recognitionRef = useRef<ExtendedSpeechRecognition | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const speechSynthesis = useRef(new SpeechSynthesisService()).current;
  const restartTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  
  // Flags simples
  const shouldContinueRef = useRef(false);
  const isCurrentlyProcessingRef = useRef(false);

  // Fonction de nettoyage simple
  const cleanup = () => {
    console.log('üßπ Nettoyage du microphone');
    shouldContinueRef.current = false;
    setIsListening(false);
    
    if (restartTimeoutRef.current) {
      clearTimeout(restartTimeoutRef.current);
      restartTimeoutRef.current = null;
    }
    
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }

    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
      } catch (error) {
        console.log('Recognition d√©j√† arr√™t√©e');
      }
    }
  };

  // Arr√™t complet
  const stopSpeaking = () => {
    console.log('üõë ARR√äT COMPLET demand√©');
    shouldContinueRef.current = false;
    setIsConversationActive(false);
    speechSynthesis.stop();
    setIsSpeaking(false);
    setIsProcessing(false);
    cleanup();
  };

  // Traitement IA simplifi√©
  const processAIResponse = async (finalTranscript: string) => {
    console.log('ü§ñ Traitement IA:', finalTranscript);
    
    if (!conversationMode || !chatGPT || !shouldContinueRef.current) {
      console.log('‚ùå Conditions non remplies pour IA');
      onTranscript(finalTranscript, "message");
      return;
    }

    if (isCurrentlyProcessingRef.current) {
      console.log('‚ùå Traitement d√©j√† en cours');
      return;
    }

    isCurrentlyProcessingRef.current = true;
    setIsProcessing(true);
    
    // Arr√™ter temporairement l'√©coute
    if (recognitionRef.current && isListening) {
      try {
        recognitionRef.current.stop();
        setIsListening(false);
      } catch (error) {
        console.log('Erreur arr√™t recognition');
      }
    }

    try {
      console.log('üì§ Envoi √† ChatGPT...');
      const response = await chatGPT.sendMessage(finalTranscript);
      
      if (!shouldContinueRef.current) {
        console.log('‚ùå Conversation arr√™t√©e pendant traitement');
        return;
      }
      
      setLastResponse(response);
      setIsSpeaking(true);
      
      console.log('üîä Synth√®se vocale...');
      speechSynthesis.speak(response, () => {
        console.log('‚úÖ Synth√®se termin√©e');
        setIsSpeaking(false);
        
        // Red√©marrer automatiquement si conversation active
        if (shouldContinueRef.current && isConversationActive) {
          console.log('üîÑ Red√©marrage automatique');
          setTimeout(() => {
            if (shouldContinueRef.current && recognitionRef.current) {
              try {
                recognitionRef.current.start();
                setIsListening(true);
              } catch (error) {
                console.error('Erreur red√©marrage:', error);
              }
            }
          }, 500);
        }
      });
      
    } catch (error) {
      console.error('‚ùå Erreur ChatGPT:', error);
      // Red√©marrer m√™me en cas d'erreur
      if (shouldContinueRef.current && isConversationActive) {
        setTimeout(() => {
          if (shouldContinueRef.current && recognitionRef.current) {
            try {
              recognitionRef.current.start();
              setIsListening(true);
            } catch (error) {
              console.error('Erreur red√©marrage apr√®s erreur:', error);
            }
          }
        }, 1000);
      }
    } finally {
      setIsProcessing(false);
      isCurrentlyProcessingRef.current = false;
    }
  };

  // D√©marrage simplifi√©
  const startListening = async () => {
    console.log('üéØ D√©marrage conversation');
    
    if (!recognitionRef.current) {
      console.log('‚ùå Reconnaissance non disponible');
      return;
    }

    // Arr√™ter synth√®se si en cours
    if (isSpeaking) {
      speechSynthesis.stop();
      setIsSpeaking(false);
    }

    try {
      // Obtenir permission micro
      if (!mediaStreamRef.current) {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaStreamRef.current = stream;
      }

      // Activer les flags
      shouldContinueRef.current = true;
      setIsConversationActive(true);

      // D√©marrer reconnaissance
      recognitionRef.current.start();
      setIsListening(true);
      
      console.log('‚úÖ Conversation d√©marr√©e');
    } catch (error) {
      console.error('‚ùå Erreur d√©marrage:', error);
      cleanup();
    }
  };

  // Arr√™t simplifi√©
  const stopListening = () => {
    console.log('üõë Arr√™t conversation');
    setIsConversationActive(false);
    stopSpeaking();
  };

  // Configuration reconnaissance - une seule fois
  useEffect(() => {
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognitionClass = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
      const recognition = new SpeechRecognitionClass() as ExtendedSpeechRecognition;
      recognitionRef.current = recognition;
      
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'fr-FR';

      recognition.onresult = (event) => {
        let finalTranscript = '';
        for (let i = event.resultIndex; i < event.results.length; i++) {
          if (event.results[i].isFinal) {
            finalTranscript += event.results[i][0].transcript;
          }
        }
        
        if (finalTranscript.trim()) {
          console.log('üéØ Transcript re√ßu:', finalTranscript);
          setTranscript(finalTranscript);
          processAIResponse(finalTranscript);
        }
      };

      recognition.onerror = (event) => {
        console.error('‚ùå Erreur reconnaissance:', event.error);
        setIsListening(false);
        
        // Red√©marrage apr√®s erreur
        if (shouldContinueRef.current && isConversationActive) {
          setTimeout(() => {
            if (shouldContinueRef.current && recognitionRef.current) {
              try {
                recognitionRef.current.start();
                setIsListening(true);
              } catch (error) {
                console.error('Erreur red√©marrage apr√®s erreur:', error);
              }
            }
          }, 1000);
        }
      };

      recognition.onend = () => {
        console.log('üèÅ Reconnaissance termin√©e');
        setIsListening(false);
        
        // Red√©marrage automatique si conversation active
        if (shouldContinueRef.current && isConversationActive && !isSpeaking && !isCurrentlyProcessingRef.current) {
          console.log('üîÑ Auto-red√©marrage');
          setTimeout(() => {
            if (shouldContinueRef.current && recognitionRef.current) {
              try {
                recognitionRef.current.start();
                setIsListening(true);
              } catch (error) {
                console.error('Erreur auto-red√©marrage:', error);
              }
            }
          }, 300);
        }
      };
    }

    return cleanup;
  }, []); // Dependency array vide - configuration une seule fois

  return {
    isListening,
    transcript,
    isProcessing,
    lastResponse,
    isSpeaking,
    isConversationActive,
    startListening,
    stopListening,
    stopSpeaking,
    cleanupMicrophone: cleanup
  };
};
